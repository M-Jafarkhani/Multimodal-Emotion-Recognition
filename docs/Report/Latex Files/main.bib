@article{greenwade93,
	author  = "George D. Greenwade",
	title   = "The {C}omprehensive {T}ex {A}rchive {N}etwork ({CTAN})",
	year    = "1993",
	journal = "TUGBoat",
	volume  = "14",
	number  = "3",
	pages   = "342--351"
}
@article{busso2008iemocap,
	author = {Carlos Busso and Murtaza Bulut and Chi-Chun Lee and Abe Kazemzadeh and Emily Mower and Samuel Kim and Jeannette N. Chang and Sungbok Lee and Shrikanth S. Narayanan},
	title = {IEMOCAP: Interactive Emotional Dyadic Motion Capture Database},
	journal = {Language Resources and Evaluation},
	volume = {42},
	number = {4},
	pages = {335--359},
	year = {2008}
}

@inproceedings{poria2019meld,
	author = {Soujanya Poria and Devamanyu Hazarika and Navonil Majumder and Gautam Naik and Erik Cambria and Rada Mihalcea},
	title = {MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	pages = {527--536},
	year = {2019},
	address = {Florence, Italy},
	publisher = {Association for Computational Linguistics}
}

@inproceedings{li2020hitrans,
	author = {Jingye Li and Donghong Ji and Fei Li and Meishan Zhang and Yijiang Liu},
	title = {HiTrans: A Transformer-Based Context- and Speaker-Sensitive Model for Emotion Detection in Conversations},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	pages = {4190--4200},
	year = {2020},
	address = {Barcelona, Spain (Online)},
	publisher = {International Committee on Computational Linguistics}
}

@inproceedings{hu2021dialoguecrn,
	author = {Dou Hu and Lingwei Wei and Xiaoyong Huai},
	title = {DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	pages = {7042--7052},
	year = {2021},
	address = {Online},
	publisher = {Association for Computational Linguistics}
}

@inproceedings{devlin2018bert,
	title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	pages={4171--4186},
	year={2019},
	organization={Association for Computational Linguistics}
}
@inproceedings{bagher-zadeh-etal-2018-multimodal,
	title = "Multimodal Language Analysis in the Wild: {CMU}-{MOSEI} Dataset and Interpretable Dynamic Fusion Graph",
	author = "Bagher Zadeh, AmirAli  and
	Liang, Paul Pu  and
	Poria, Soujanya  and
	Cambria, Erik  and
	Morency, Louis-Philippe",
	editor = "Gurevych, Iryna  and
	Miyao, Yusuke",
	booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2018",
	address = "Melbourne, Australia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P18-1208",
	doi = "10.18653/v1/P18-1208",
	pages = "2236--2246",
	abstract = "Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.",
}
@article{Zadeh2016MOSIMC,
	title={MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos},
	author={Amir Zadeh and Rowan Zellers and Eli Pincus and Louis-Philippe Morency},
	journal={ArXiv},
	year={2016},
	volume={abs/1606.06259},
	url={https://api.semanticscholar.org/CorpusID:13978043}
}

@misc{Baltruaitis2017MultimodalML,
	title={Multimodal Machine Learning: A Survey and Taxonomy}, 
	author={Tadas Baltru≈°aitis and Chaitanya Ahuja and Louis-Philippe Morency},
	year={2017},
	eprint={1705.09406},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1705.09406}, 
}

@inproceedings{Ngiam2011MultimodalDL,
	title={Multimodal Deep Learning},
	author={Jiquan Ngiam and Aditya Khosla and Mingyu Kim and Juhan Nam and Honglak Lee and A. Ng},
	booktitle={International Conference on Machine Learning},
	year={2011},
	url={https://api.semanticscholar.org/CorpusID:352650}
}
@INPROCEEDINGS {Sun2019VideoBERT,
	author = { Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia },
	booktitle = { 2019 IEEE/CVF International Conference on Computer Vision (ICCV) },
	title = {{ VideoBERT: A Joint Model for Video and Language Representation Learning }},
	year = {2019},
	volume = {},
	ISSN = {},
	pages = {7463-7472},
	keywords = {Bit error rate;Visualization;Task analysis;Data models;Predictive models;Linguistics;Training},
	doi = {10.1109/ICCV.2019.00756},
	url = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00756},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month =Nov}
@inproceedings{tsai-etal-2019-multimodal,
	title = "Multimodal Transformer for Unaligned Multimodal Language Sequences",
	author = "Tsai, Yao-Hung Hubert  and
	Bai, Shaojie  and
	Liang, Paul Pu  and
	Kolter, J. Zico  and
	Morency, Louis-Philippe  and
	Salakhutdinov, Ruslan",
	editor = "Korhonen, Anna  and
	Traum, David  and
	M{\`a}rquez, Llu{\'\i}s",
	booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2019",
	address = "Florence, Italy",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P19-1656",
	doi = "10.18653/v1/P19-1656",
	pages = "6558--6569",
	abstract = "Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.",
}

@inproceedings{Liu2018EfficientLM,
	title={Efficient Low-rank Multimodal Fusion With Modality-Specific Factors},
	author={Zhun Liu and Ying Shen and Varun Bharadhwaj Lakshminarasimhan and Paul Pu Liang and Amir Zadeh and Louis-Philippe Morency},
	booktitle={Annual Meeting of the Association for Computational Linguistics},
	year={2018},
	url={https://api.semanticscholar.org/CorpusID:44131945}
}
@inproceedings{zadeh-etal-2017-tensor,
	title = "Tensor Fusion Network for Multimodal Sentiment Analysis",
	author = "Zadeh, Amir  and
	Chen, Minghai  and
	Poria, Soujanya  and
	Cambria, Erik  and
	Morency, Louis-Philippe",
	editor = "Palmer, Martha  and
	Hwa, Rebecca  and
	Riedel, Sebastian",
	booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
	month = sep,
	year = "2017",
	address = "Copenhagen, Denmark",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D17-1115",
	doi = "10.18653/v1/D17-1115",
	pages = "1103--1114",
	abstract = "Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion Networks, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice. In the experiments, our model outperforms state-of-the-art approaches for both multimodal and unimodal sentiment analysis.",
}

@inproceedings{barezi-fung-2019-modality,
	title = "Modality-based Factorization for Multimodal Fusion",
	author = "Barezi, Elham J.  and
	Fung, Pascale",
	editor = "Augenstein, Isabelle  and
	Gella, Spandana  and
	Ruder, Sebastian  and
	Kann, Katharina  and
	Can, Burcu  and
	Welbl, Johannes  and
	Conneau, Alexis  and
	Ren, Xiang  and
	Rei, Marek",
	booktitle = "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",
	month = aug,
	year = "2019",
	address = "Florence, Italy",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/W19-4331",
	doi = "10.18653/v1/W19-4331",
	pages = "260--269",
	abstract = "We propose a novel method, Modality-based Redundancy Reduction Fusion (MRRF), for understanding and modulating the relative contribution of each modality in multimodal inference tasks. This is achieved by obtaining an (\textit{M}+1)-way tensor to consider the high-order relationships between \textit{M} modalities and the output layer of a neural network model. Applying a modality-based tensor factorization method, which adopts different factors for different modalities, results in removing information present in a modality that can be compensated by other modalities, with respect to model outputs. This helps to understand the relative utility of information in each modality. In addition it leads to a less complicated model with less parameters and therefore could be applied as a regularizer avoiding overfitting. We have applied this method to three different multimodal datasets in sentiment analysis, personality trait recognition, and emotion recognition. We are able to recognize relationships and relative importance of different modalities in these tasks and achieves a 1{\%} to 4{\%} improvement on several evaluation measures compared to the state-of-the-art for all three tasks.",
}

@article{Wu2022,
	author    = {Jie Wu and Tingting Zhu and Xiaobo Zheng and Chunyuan Wang},
	title     = {Multi-Modal Sentiment Analysis Based on Interactive Attention Mechanism},
	journal   = {Applied Sciences},
	year      = {2022},
	volume    = {12},
	number    = {16},
	pages     = {8174},
	doi       = {10.3390/app12168174},
	url       = {https://www.mdpi.com/2076-3417/12/16/8174}
}

@misc{FactorizedMult,
	title={Learning Factorized Multimodal Representations}, 
	author={Yao-Hung Hubert Tsai and Paul Pu Liang and Amir Zadeh and Louis-Philippe Morency and Ruslan Salakhutdinov},
	year={2019},
	eprint={1806.06176},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1806.06176}, 
}

@inproceedings{Pham2019MCTN,
	author = {Pham, Hai and Liang, Paul Pu and Manzini, Thomas and Morency, Louis-Philippe and P\'{o}czos, Barnab\'{a}s},
	title = {Found in translation: learning robust joint representations by cyclic translations between modalities},
	year = {2019},
	isbn = {978-1-57735-809-1},
	publisher = {AAAI Press},
	url = {https://doi.org/10.1609/aaai.v33i01.33016892},
	doi = {10.1609/aaai.v33i01.33016892},
	abstract = {Multimodal sentiment analysis is a core research area that studies speaker sentiment expressed from the language, visual, and acoustic modalities. The central challenge in multimodal learning involves inferring joint representations that can process and relate information from these modalities. However, existing work learns joint representations by requiring all modalities as input and as a result, the learned representations may be sensitive to noisy or missing modalities at test time. With the recent success of sequence to sequence (Seq2Seq) models in machine translation, there is an opportunity to explore new ways of learning joint representations that may not require all input modalities at test time. In this paper, we propose a method to learn robust joint representations by translating between modalities. Our method is based on the key insight that translation from a source to a target modality provides a method of learning joint representations using only the source modality as input. We augment modality translations with a cycle consistency loss to ensure that our joint representations retain maximal information from all modalities. Once our translation model is trained with paired multimodal data, we only need data from the source modality at test time for final sentiment prediction. This ensures that our model remains robust from perturbations or missing information in the other modalities. We train our model with a coupled translation-prediction objective and it achieves new state-of-the-art results on multimodal sentiment analysis datasets: CMU-MOSI, ICT-MMMO, and YouTube. Additional experiments show that our model learns increasingly discriminative joint representations with more input modalities while maintaining robustness to missing or perturbed modalities.},
	booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
	articleno = {846},
	numpages = {8},
	location = {Honolulu, Hawaii, USA},
	series = {AAAI'19/IAAI'19/EAAI'19}
}